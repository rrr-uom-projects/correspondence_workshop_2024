{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning-based shape correspondence workshop (June 2024)\n",
    "### Overview:\n",
    "\n",
    "In this hands-on tutorial we're going to go therough the process of setting up, training and evaluating the results produced by a popular learning-based correspondence method.\n",
    "\n",
    "For this tutorial we shall use the SHREC'20 dataset of 3D animal shapes: http://robertodyke.com/shrec2020/index2.html\n",
    "\n",
    "We will be using the \"Neuromorph\" correspondence model designed by Eisenberger et al. that was described briefly in the talk this morning. Paper: https://arxiv.org/pdf/2106.09431 Original code: https://github.com/facebookresearch/neuromorph\n",
    "\n",
    "We'll be going through this tutorial step-by-step and please feel free to ask any questions along the way.\n",
    "\n",
    "---\n",
    "### Contents\n",
    "The steps in this notebook are as follows:\n",
    "\n",
    "0. Install prerequisites and set up the google colab environment\n",
    "1. Explore the dataset\n",
    "2. Pre-processing - Supervised vs. unsupervised learning for correspondence models\n",
    "3. Setting up a learning-based correspondence model (Neuromorph)\n",
    "4. Training the correspondence model\n",
    "5. Evaluation - geodesic distance and landmarks\n",
    "6. Exploration of limitations and possible solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Colab \n",
    "Colab is a free ML playground from google. It allows you free access to limited resources, including a GPU and some storage space. Even though the limits are quite small: ~50GB disk, 12GB RAM & a T4 GPU, you can do some pretty cool stuff with it. \n",
    "\n",
    "__You will need a Google account to sign into it.__\n",
    "\n",
    "*Important*:\n",
    "\n",
    "We are going to be training a Convolutional Neural Network (CNN), so we need to get a GPU. To do this, click \"Runtime\" in the menu across the top of the colab page, then select \"Change Runtime Type\". From the dropdown, select GPU and click Save. The runtime will then reboot and you should have a GPU. To find out what you got, run the cell below this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find out what GPU we got (and make sure we actually have one!)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download all the data and network library! This may take a little while...\n",
    "\n",
    "## Processed data\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !wget -O /content/data_directory.tar.gz https://www.dropbox.com/scl/fi/hgjapycrcuste048yh6b9/data_directory.tar.gz?rlkey=c3eerq4g2xnf7faw8olkjhmp6&st=bpgzfl0l&dl=0 \n",
    "    !tar -xzf /content/data_directory.tar.gz -C /content/\n",
    "    !rm /content/data_directory.tar.gz\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    !wget -O ./data_directory.tar.gz https://www.dropbox.com/scl/fi/hgjapycrcuste048yh6b9/data_directory.tar.gz?rlkey=c3eerq4g2xnf7faw8olkjhmp6&st=khq2k6i9&dl=0\n",
    "    !tar -xzf ./data_directory.tar.gz -C ./\n",
    "    !rm ./data_directory.tar.gz\n",
    "\n",
    "# network library files and images\n",
    "!git clone https://github.com/edhendo/test_workshop.git\n",
    "!cp -r test_workshop/neuromorph_adapted/ .\n",
    "!rm -r test_workshop/\n",
    "\n",
    "# pretrained model weights\n",
    "if IN_COLAB:\n",
    "    !wget -O /content/ckpt_ep229.pth https://www.dropbox.com/scl/fi/bj2564h5oyyu0c4d9v0l6/ckpt_ep229.pth?rlkey=sktsapnqmn5h8w5ngwur9v0r8&st=ed9apxpf&dl=0\n",
    "else:\n",
    "    !wget -O ./ckpt_ep229.pth https://www.dropbox.com/scl/fi/bj2564h5oyyu0c4d9v0l6/ckpt_ep229.pth?rlkey=sktsapnqmn5h8w5ngwur9v0r8&st=ed9apxpf&dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open3d\n",
    "!pip install pyvista==0.37.0\n",
    "!apt install libgl1-mesa-glx xvfb\n",
    "!pip install trame-vtk\n",
    "!pip install ipywidgets\n",
    "!pip install pygeodesic\n",
    "!pip install torch_geometric\n",
    "!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The dataset (SHREC'20 3D meshes of animals)\n",
    "\n",
    "To start with let's explore the dataset. In this tutorial we will be looking at the SHREC 2020 dataset containing non-isometric deformations. This dataset consists of the 3D shapes of 14 different animals. The 3D shapes themselves are constructed of triangular meshes. \n",
    "\n",
    "Let's first visualise a couple of these 3D animal shapes to get an idea of what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "import numpy as np\n",
    "import open3d as o3d        # open3d is a very powerful python library for 3D shape operations and manipulation\n",
    "\n",
    "mesh = o3d.io.read_triangle_mesh(\"data/meshes/camel.obj\")\n",
    "vertices = np.asarray(mesh.vertices)\n",
    "triangles = np.asarray(mesh.triangles)\n",
    "print(f\"Vertices:    shape: {vertices.shape}\")\n",
    "print(vertices, \"\\n\")\n",
    "print(f\"Triangles:    shape: {triangles.shape}\")\n",
    "print(triangles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Open3D visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = o3d.io.read_triangle_mesh(\"data/meshes/camel.obj\")\n",
    "vertices = np.asarray(mesh.vertices)\n",
    "vertices = vertices[:, [0,2,1]] \n",
    "mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "o3d.visualization.draw_plotly([mesh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a dedicated package built for 3D visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista              # pyvista the the library we will use to visualise the 3D models\n",
    "pyvista.start_xvfb()        # virtual frame buffer necessary for colab and notebooks (linux only - remove if running locally on a different OS)\n",
    "\n",
    "# helper function to convert to pyvista data type\n",
    "def pyvistarise(verts, triangles):\n",
    "    return pyvista.PolyData(verts, np.insert(triangles, 0, 3, axis=1), deep=True, n_faces=len(triangles))\n",
    "\n",
    "mesh = o3d.io.read_triangle_mesh(\"data/meshes/camel.obj\")\n",
    "verts = np.asarray(mesh.vertices)[:, [0,2,1]]\n",
    "triangles = np.asarray(mesh.triangles)\n",
    "pyv_mesh = pyvistarise(verts, triangles)\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "plotter.add_mesh(pyv_mesh, show_edges=True)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the some of the other models available in this dataset:\n",
    "\n",
    "Make sure to look at the bear! What's different about this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of: [\"bear\", \"bison\", \"cow\", \"camel\", \"dog\", \"elephant\", \"giraffe\", \"hippo\", \"leopard\", \"pig\", \"rhino\"] \n",
    "animal = \"pig\"\n",
    "mesh = o3d.io.read_triangle_mesh(f\"data/meshes/{animal}.obj\")\n",
    "verts = np.asarray(mesh.vertices)[:, [0,2,1]]\n",
    "triangles = np.asarray(mesh.triangles)\n",
    "pyv_mesh = pyvistarise(verts, triangles)\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "plotter.add_mesh(pyv_mesh, show_edges=True)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side note: reducing the size of large meshes\n",
    "Sometimes it's useful to reduce the size/resolution of the 3D models (e.g. for computational requirements/efficiency). \n",
    "\n",
    "Open3D has good tools for this, you can play with the examples below to see how these work. How does a model look if you reduce the number of triangles to 10,000, 1,000, 100?\n",
    "\n",
    "Each of the shape models in the dataset we're using today has a maximum of 20,000 which is reasonable for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = \"pig\"\n",
    "mesh = o3d.io.read_triangle_mesh(f\"data/meshes/{animal}.obj\")\n",
    "\n",
    "mesh = mesh.simplify_quadric_decimation(target_number_of_triangles=1000)\n",
    "#mesh = mesh.filter_smooth_taubin(number_of_iterations=10)             # additionally smoothing can be useful when reducing the resolution of large meshes\n",
    "\n",
    "verts = np.asarray(mesh.vertices)[:, [0,2,1]]\n",
    "triangles = np.asarray(mesh.triangles)\n",
    "pyv_mesh = pyvistarise(verts, triangles)\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "plotter.add_mesh(pyv_mesh, show_edges=True)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Pre-processing - Supervised vs. un-supervised learning for correspondence models\n",
    "\n",
    "- What is the difference between supervised and un-supervised learning?\n",
    "- For learning-based correspondence models unsupervised learning tends to be most regularly used.\n",
    "- What could be the issue of using supervised learning when training a correspondence model?\n",
    "\n",
    "We need some \"metric of matching\" to drive the learning process and train the model to make reasonable correspondence predictions.\n",
    "\n",
    "To create this metric we're going to use geodesic distances or paths. The image below shows geodesic distances from the tigers mouth in different colours.\n",
    "\n",
    "![image of geodesic tiger](images/tiger_geodesic_dists.jpg)\n",
    "\n",
    "Let's plot a geodesic path on one of shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeodesic.geodesic as geodesic      # import package to calculate geodesic distances\n",
    "\n",
    "mesh = o3d.io.read_triangle_mesh(\"data/meshes/camel.obj\")\n",
    "vertices = np.asarray(mesh.vertices)[:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "triangles = np.asarray(mesh.triangles)\n",
    "geo_alg = geodesic.PyGeodesicAlgorithmExact(vertices, triangles)\n",
    "source_index = 4000\n",
    "target_index = 1500\n",
    "distances, _ = geo_alg.geodesicDistances(np.array([source_index]))\n",
    "distance, path = geo_alg.geodesicDistance(source_index, target_index)\n",
    "\n",
    "pyv_mesh = pyvistarise(vertices, triangles)\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "\n",
    "sargs = dict(title_font_size=28, label_font_size=26, shadow=False, n_labels=5, italic=False, fmt=\"%.1f\", font_family=\"arial\", color='black', bold=False, title=\"Geodesic distance\")\n",
    "plotter.add_mesh(pyv_mesh, show_edges=False, scalars=distances, cmap=\"YlOrRd\", scalar_bar_args=sargs)\n",
    "plotter.add_lines(path, color='b', width=5)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use these to learn correspondences between different shapes?\n",
    "\n",
    "- Formulate a loss function based on conservation of geodesic distances:\n",
    "- i.e. compare distance between two points on mesh X and their allocated corresponding points on mesh Y\n",
    "- minimise the difference between the two distances\n",
    "\n",
    "![image of s](images/geodesic_loss_component.png)\n",
    "\n",
    "D_x and D_y are matrices of the geodesic distances (see example below) and Pi is the correspondence assignment\n",
    " \n",
    "For full details please see Eisenberger et al. (https://arxiv.org/pdf/2106.09431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short example plotting a geodesic matrix for a single shape\n",
    "import matplotlib.pyplot as plt\n",
    "camel_geodesic_distances = np.load(\"data/geodesic_distances/camel.npy\")\n",
    "plt.imshow(camel_geodesic_distances, cmap=\"magma\")\n",
    "plt.title(\"Geodesic distances\")\n",
    "plt.xlabel(\"Index i\")\n",
    "plt.ylabel(\"Index j\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing step\n",
    "\n",
    "To start with we need to compute these geodesic matrices for each shape!\n",
    "\n",
    "N.B. This step is MASSIVELY computationally expensive and takes a few hours with a good CPU - fortunately the matrices have already been computed and are located in \"./data/geodesic_distances/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeodesic.geodesic as geodesic\n",
    "from neuromorph_adapted.utils.utils import getFiles\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_geodesic_distances(vertices, triangles):    # function to compute geodesic distances between all pairs of vertices\n",
    "    n_vertices = vertices.shape[0]\n",
    "    geo_alg = geodesic.PyGeodesicAlgorithmExact(vertices, triangles)\n",
    "    D = np.zeros((n_vertices, n_vertices), dtype=np.float16)\n",
    "    for source_index in tqdm(range(n_vertices)):\n",
    "        D[source_index], _ = geo_alg.geodesicDistances(np.array([source_index]))\n",
    "    try:\n",
    "        assert (D - D.T < 1e-4).all()\n",
    "    except AssertionError:\n",
    "        print(f\"Assertion error - max value: {np.abs(D - D.T).max()}\")\n",
    "        # If nan -> It's likely the mesh is not a single connected component -> run some connected component analysis\n",
    "        return None\n",
    "    return D\n",
    "\n",
    "data_folder = \"data/meshes\"\n",
    "output_folder = \"data/geodesic_distances\"\n",
    "\n",
    "fnames = getFiles(data_folder)\n",
    "fname = fnames[0]\n",
    "mesh = o3d.io.read_triangle_mesh(join(data_folder, fname))\n",
    "vertices = np.asarray(mesh.vertices)\n",
    "triangles = np.asarray(mesh.triangles)\n",
    "D = compute_geodesic_distances(vertices, triangles)\n",
    "if D is not None:\n",
    "    print(f\"Computed geodesic distances for {fname}\")\n",
    "else:\n",
    "    print(f\"Failed to compute geodesic distances for {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT!\n",
    "Need to restart the virtual frame buffer after KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyvista.start_xvfb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating full matrices takes way too long! So they've been computed ahead of time and are located in \"./data/geodesic_distances/\"\n",
    "\n",
    "Cool what's next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration loss and regularisation\n",
    "\n",
    "A loss function based on geodesic distances isn't enough - particularly when the shapes are non-isometric!\n",
    "\n",
    "![non-isometric example](images/non_isometric_example.png)\n",
    "\n",
    "To overcome this Eisenberger et al. introduce use a model sucture which additionally deforms and warps the shapes to overlap and integrate aregistration loss based on chamfer matching.\n",
    "\n",
    "It is possible to attain perfect registration by deforming a shape wildly: (blue: target, red: source)\n",
    "\n",
    "LEFT: Before deformation --- CENTRE: deformation with regularisation --- RIGHT: deformation without regularisation\n",
    "\n",
    "![mad fish registration](images/2d_chamfer_matching_registration_example.png)\n",
    "\n",
    "Additionally need a way of regularising the allowed deformations\n",
    "- add a local distortion metric to minimise distortion of triangles through interpolation.\n",
    "- for details see Eisenberger et al. (https://arxiv.org/pdf/2106.09431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Setting up a learning-based correspondence model (Neuromorph)\n",
    "\n",
    "Here's a look at the Neuromorph model architecture from their paper: (https://arxiv.org/pdf/2106.09431)\n",
    "\n",
    "![Neuromorph architecture](images/neuromorph_architecture.png)\n",
    "\n",
    "The Neuromorph model is formed of two components: twin feature extracting networks with shared weights and an interpolator. The feature-extracting portion consists of two networks with shared features which receive two meshes as input. The encoded shape features are matched using matrix multiplication to produce a correspondence matrix between the input meshes. The correspondence matrix is used to produce a vector which contains the offset between source vertices and their corresponding counterparts in the target mesh. This offset vector provides part of the input to the interpolator, along with the original source vertices and a time-step encoding to provide the number of intermediate steps along which to interpolate the deformation. The interpolator outputs a deformation vector for these time-steps for each vertex in the source mesh. The feature extractor and interpolator have identical graph neural network architectures, consisting of repeating residual EdgeConv layers. \n",
    "\n",
    "Neuromorph relies on EdgeConv layers: (https://arxiv.org/pdf/1801.07829)\n",
    "\n",
    "![edge conv](images/edge_conv_fig.png)\n",
    "\n",
    "EdgeConv captures local geometric structure generating edge features that describe the relationship between a point and its neighbors. Stacking multiple EdgeConv layers enable you to learn global shape properties. (see for more details: https://medium.com/@sanketgujar95/dynamic-graph-cnn-edge-conv-2582c3eb18d8)\n",
    "\n",
    "---\n",
    "\n",
    " - The primary intuition behind success of the Neuromorph architecture is that correspondence and interpolation are interdependent tasks that complement each other when optimised in an end-to-end fashion.\n",
    " - In this tutorial we're only really going to explore the predicted correspondences but the model outputs interpolation sequences between shapes too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set up the correspondence model and make some predictions with a randomly initialised network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant portions of the correspondence model library\n",
    "from neuromorph_adapted.model.interpolation_net import *\n",
    "from neuromorph_adapted.utils.arap import ArapInterpolationEnergy\n",
    "from neuromorph_adapted.data.data import *\n",
    "from neuromorph_adapted.utils.utils import ParametersBase\n",
    "import pickle\n",
    "\n",
    "# load the model and some mesh data\n",
    "data_folder = \"data/meshes\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "source_mesh = o3d.io.read_triangle_mesh(join(data_folder, \"bison.obj\"))\n",
    "target_mesh = o3d.io.read_triangle_mesh(join(data_folder, \"cow.obj\"))\n",
    "\n",
    "\n",
    "# This class is used to set particular hyper-parameters of the correspondence model, we'll add a few more in the next step\n",
    "class NetworkParameters(ParametersBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = 0\n",
    "        self.hidden_dim = 64\n",
    "\n",
    "def run_correpondence_inference(model, source_mesh, target_mesh):\n",
    "    \n",
    "    def convert_to_batch(mesh):\n",
    "        verts = torch.tensor(np.asarray(mesh.vertices).astype(np.float32))\n",
    "        shift = torch.mean(verts, dim=0)\n",
    "        verts = verts - shift\n",
    "        #verts = verts / torch.max(torch.abs(verts) # some of the shapes in this dataset are very different sizes and it helps to normalise the physical size\n",
    "        triangles = torch.tensor(np.asarray(mesh.triangles))\n",
    "        return {\"verts\": verts, \"triangles\": triangles, \"shift\": shift}\n",
    "    \n",
    "    # convert raw mesh data to batch dictionary for the model (just for consistency later)\n",
    "    source_data = convert_to_batch(source_mesh)\n",
    "    target_data = convert_to_batch(target_mesh)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        source_shape = batch_to_shape(source_data)\n",
    "        target_shape = batch_to_shape(target_data)\n",
    "\n",
    "        point_pred = model.get_pred(source_shape, target_shape)     # run inference between the two shapes\n",
    "        point_pred = point_pred.cpu().numpy()                       \n",
    "\n",
    "        corr_out = model.match(source_shape, target_shape)          # obtain correspondence prediction (logits / probabilities)\n",
    "        assignment = corr_out.argmax(dim=1).cpu().numpy()           # determine correspondence mapping by finding the maximum probability for each vertex (fairly crude)\n",
    "        assignmentinv = corr_out.argmax(dim=0).cpu().numpy()        # similarly find the reverse mapping\n",
    "\n",
    "        source_verts = source_shape.verts.cpu().numpy()\n",
    "        target_verts = target_shape.verts.cpu().numpy()\n",
    "        triangles_x = source_shape.triangles.cpu().numpy()\n",
    "        triangles_y = target_shape.triangles.cpu().numpy()\n",
    "\n",
    "        result = {}\n",
    "        result[\"assignment\"] = assignment\n",
    "        result[\"assignmentinv\"] = assignmentinv\n",
    "        result[\"X\"] = {\"verts\": source_verts, \"triangles\": triangles_x, \"shift\": source_data[\"shift\"]}\n",
    "        result[\"Y\"] = {\"verts\": target_verts, \"triangles\": triangles_y, \"shift\": target_data[\"shift\"]}\n",
    "        result[\"interpolation_verts\"] = point_pred\n",
    "        result[\"raw_correspondence_predictions\"] = corr_out.cpu().numpy()\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Deformation model based on the \"As-rigid-as-possible\" energy formulation (Olga Sorkine and Marc Alexa, 2007)\n",
    "interpolation_energy = ArapInterpolationEnergy()\n",
    "# Create full correspondence model (random initialisation)\n",
    "correspondence_model = InterpolationModGeoEC(interpolation_energy, NetworkParameters()).to(device)\n",
    "\n",
    "result = run_correpondence_inference(correspondence_model, source_mesh, target_mesh)\n",
    "# save the result\n",
    "with open(\"results/random_init_correspondence_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)\n",
    "\n",
    "# display outputs (visualise)\n",
    "source_verts = result[\"X\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "target_verts = result[\"Y\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "triangles_x = result[\"X\"][\"triangles\"]\n",
    "triangles_y = result[\"Y\"][\"triangles\"]\n",
    "\n",
    "# offset the target mesh for the visualsation\n",
    "target_verts = target_verts + np.array([1,0,0])\n",
    "\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "\n",
    "# Forward assignment\n",
    "# set colours on target mesh corresponding with xyz position\n",
    "target_colours = target_verts - np.min(target_verts, axis=0)\n",
    "target_colours = target_colours / np.max(target_colours, axis=0)\n",
    "target_colours = np.concatenate([target_colours, np.ones((len(target_colours), 1))], axis=1)\n",
    "# set colours on source mesh according to the predicted correspondence assignment\n",
    "assignment = result[\"assignment\"]\n",
    "source_colours = target_colours[assignment]\n",
    "\n",
    "# add points\n",
    "plotter.add_points(target_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=target_colours, rgb=True)\n",
    "plotter.add_points(source_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=source_colours, rgb=True)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat interestingly, an untrained model with randomly initialised weights can produce plausible results.\n",
    "\n",
    " - Attaiki et al. showed a neural network itself can act as a powerful prior for shape correspondence problems.\n",
    " - Produce pointwise features that capture local geometry and lead to well-structured maps between shapes.\n",
    " - https://arxiv.org/abs/2301.05839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training the correspondence model\n",
    "\n",
    "Now let's actually train the network on a small subset of data to see if the correspondence results improve!\n",
    "\n",
    "To start with we'll train for a single epoch on all animals apart from our testing pair, the cow and the bison. \n",
    "\n",
    "This yields 9^2 = 81 training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code\n",
    "from neuromorph_adapted.model.interpolation_net import *\n",
    "from neuromorph_adapted.utils.arap import ArapInterpolationEnergy\n",
    "from neuromorph_adapted.data.data import *\n",
    "from neuromorph_adapted.utils.utils import ParametersBase, k_fold_split_train_val_test, getFiles\n",
    "\n",
    "class NetworkParameters(ParametersBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr = 1e-4\n",
    "        self.num_it = 1\n",
    "        self.batch_size = 16\n",
    "        self.num_timesteps = 0\n",
    "        self.hidden_dim = 64\n",
    "        # the lambda values here control the weights of the three loss functions we described previously\n",
    "        self.lambd = 1          # registration loss\n",
    "        self.lambd_geo = 100    # geodesic distance conservation loss\n",
    "        self.lambd_arap = 1     # regularisation\n",
    "\n",
    "        self.log_freq = 1\n",
    "        self.val_freq = 1\n",
    "\n",
    "        self.log = True\n",
    "\n",
    "def create_correspondence_model(dataset, dataset_val=None, time_stamp=None, description=\"\", param=NetworkParameters(), folder_weights_load=None):\n",
    "    if time_stamp is None:\n",
    "        time_stamp = get_timestr()\n",
    "\n",
    "    # Deformation model based on the \"As-rigid-as-possible\" energy formulation (Olga Sorkine and Marc Alexa, 2007)\n",
    "    interpol_energy = ArapInterpolationEnergy()\n",
    "\n",
    "    # Correspondence model\n",
    "    interpol_module = InterpolationModGeoEC(interpol_energy, param).to(device)\n",
    "\n",
    "    \n",
    "    preproc_mods = []\n",
    "    preproc_mods.append(PreprocessRotateSame(dataset.axis))\n",
    "    \n",
    "    settings_module = timestep_settings(increase_thresh=3)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
    "    if dataset_val is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(dataset_val, shuffle=False)\n",
    "    else:\n",
    "        val_loader = None\n",
    "\n",
    "    interpol = InterpolNet(interpol_module, train_loader, val_loader=val_loader, time_stamp=time_stamp, description=description, preproc_mods=preproc_mods, settings_module=settings_module)\n",
    "\n",
    "    if folder_weights_load is not None:\n",
    "        interpol.load_self(save_path(folder_str=folder_weights_load))\n",
    "\n",
    "    return interpol\n",
    "\n",
    "data_folder = \"./data/\"\n",
    "\n",
    "# # determine which meshes to use - this code would be useful if we were performing a proper cross-validaion\n",
    "# all_fnames = getFiles(join(data_folder, \"meshes\"))\n",
    "# train_inds, val_inds, _ = k_fold_split_train_val_test(dataset_size=len(all_fnames), fold_num=1, seed=1004)\n",
    "# train_fnames = [all_fnames[i].replace('.obj','') for i in train_inds]\n",
    "# val_fnames = [all_fnames[i].replace('.obj','') for i in val_inds]\n",
    "\n",
    "train_fnames = [\"bear\", \"camel\", \"dog\", \"elephant\", \"giraffe\", \"hippo\", \"leopard\", \"pig\", \"rhino\"] \n",
    "#val_fnames = [\"bison\", \"cow\"]\n",
    "\n",
    "dataset_train = general_dataset(data_folder, fnames=train_fnames, load_dist_mat=True)\n",
    "#dataset_val = general_dataset(data_folder, fnames=val_fnames, load_dist_mat=True)\n",
    "\n",
    "correspondence_model = create_correspondence_model(dataset_train, dataset_val=None, description=f\"test1\")\n",
    "correspondence_model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the trained correspondence model\n",
    "\n",
    "Now let's retry inference with the model that has been trained for a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and some mesh data\n",
    "data_folder = \"data/meshes\"\n",
    "source_mesh = o3d.io.read_triangle_mesh(join(data_folder, \"bison.obj\"))\n",
    "target_mesh = o3d.io.read_triangle_mesh(join(data_folder, \"cow.obj\"))\n",
    "\n",
    "# inference\n",
    "result = run_correpondence_inference(correspondence_model.interp_module, source_mesh, target_mesh)\n",
    "\n",
    "# display outputs (visualise)\n",
    "source_verts = result[\"X\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "target_verts = result[\"Y\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "triangles_x = result[\"X\"][\"triangles\"]\n",
    "triangles_y = result[\"Y\"][\"triangles\"]\n",
    "\n",
    "# offset\n",
    "target_verts = target_verts + np.array([1,0,0])\n",
    "\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "\n",
    "# Forward assignment\n",
    "# set colours on target mesh corresponding with xyz position\n",
    "target_colours = target_verts - np.min(target_verts, axis=0)\n",
    "target_colours = target_colours / np.max(target_colours, axis=0)\n",
    "target_colours = np.concatenate([target_colours, np.ones((len(target_colours), 1))], axis=1)\n",
    "# set colours on source mesh according to the predicted correspondence assignment\n",
    "assignment = result[\"assignment\"]\n",
    "source_colours = target_colours[assignment]\n",
    "\n",
    "# add points\n",
    "plotter.add_points(target_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=target_colours, rgb=True)\n",
    "plotter.add_points(source_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=source_colours, rgb=True)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single epoch and with such a small dataset it's unlikely that the model will get too much better unfortunately.\n",
    "\n",
    "### Using a pre-trained model\n",
    "\n",
    "Instead try loading the weights from a model that has been trained on the same dataset but for hundreds of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromorph_adapted.model.interpolation_net import *\n",
    "from neuromorph_adapted.utils.arap import ArapInterpolationEnergy\n",
    "from neuromorph_adapted.data.data import *\n",
    "from neuromorph_adapted.utils.utils import ParametersBase\n",
    "\n",
    "class NetworkParameters(ParametersBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr = 1e-4\n",
    "        self.num_it = 1\n",
    "        self.batch_size = 16\n",
    "        self.num_timesteps = 0\n",
    "        self.hidden_dim = 128\n",
    "        self.lambd = 1\n",
    "        self.lambd_geo = 100\n",
    "        self.lambd_arap = 1\n",
    "\n",
    "        self.log_freq = 1\n",
    "        self.val_freq = 1\n",
    "\n",
    "        self.log = True\n",
    "\n",
    "interpol_energy = ArapInterpolationEnergy()\n",
    "correspondence_model = InterpolationModGeoEC(interpol_energy, param=NetworkParameters()).to(device)\n",
    "correspondence_model.load_self(\"./\", num_epoch=229)\n",
    "\n",
    "# load the model and some mesh data\n",
    "source_mesh = o3d.io.read_triangle_mesh(\"data/meshes/bison.obj\")\n",
    "target_mesh = o3d.io.read_triangle_mesh(\"data/meshes/cow.obj\")\n",
    "\n",
    "# inference\n",
    "result = run_correpondence_inference(correspondence_model, source_mesh, target_mesh)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/trained_correspondence_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)\n",
    "\n",
    "# display outputs (visualise)\n",
    "source_verts = result[\"X\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "target_verts = result[\"Y\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "triangles_x = result[\"X\"][\"triangles\"]\n",
    "triangles_y = result[\"Y\"][\"triangles\"]\n",
    "\n",
    "# offset the target shape for the visualisation\n",
    "target_verts = target_verts + np.array([1,0,0])\n",
    "\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "\n",
    "# Forward assignment\n",
    "# set colours on target mesh corresponding with xyz position\n",
    "target_colours = target_verts - np.min(target_verts, axis=0)\n",
    "target_colours = target_colours / np.max(target_colours, axis=0)\n",
    "target_colours = np.concatenate([target_colours, np.ones((len(target_colours), 1))], axis=1)\n",
    "# set colours on source mesh according to the predicted correspondence assignment\n",
    "assignment = result[\"assignment\"]\n",
    "source_colours = target_colours[assignment]\n",
    "\n",
    "# add points\n",
    "plotter.add_points(target_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=target_colours, rgb=True)\n",
    "plotter.add_points(source_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=source_colours, rgb=True)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Numerical evaluation\n",
    "\n",
    "Creating pretty picture like this is all well and good but we also eed some more concrete evaluation measures to be able to tell if the model is doing well or not and to compare different model hyperparameters or architectures!\n",
    "\n",
    "### To create some evaluation metrics we're going to use some manually annotated landmarks and the geodesic distances we computed before.\n",
    "\n",
    "To start with let's look at these landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "# display outputs (visualise)\n",
    "source_verts = result[\"X\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "target_verts = result[\"Y\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "triangles_x = result[\"X\"][\"triangles\"]\n",
    "triangles_y = result[\"Y\"][\"triangles\"]\n",
    "\n",
    "# offset\n",
    "target_verts = target_verts + np.array([1,0,0])\n",
    "\n",
    "pyvista.global_theme.background = 'white'\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "\n",
    "# Forward assignment\n",
    "# set colours on target mesh corresponding with xyz position\n",
    "target_colours = target_verts - np.min(target_verts, axis=0)\n",
    "target_colours = target_colours / np.max(target_colours, axis=0)\n",
    "target_colours = np.concatenate([target_colours, np.ones((len(target_colours), 1))], axis=1)\n",
    "# set colours on source mesh according to the predicted correspondence assignment\n",
    "assignment = result[\"assignment\"]\n",
    "source_colours = target_colours[assignment]\n",
    "\n",
    "# add points\n",
    "plotter.add_points(target_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=target_colours, rgb=True)\n",
    "plotter.add_points(source_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=source_colours, rgb=True)\n",
    "\n",
    "bison_landmark_indices = sio.loadmat(\"data/ground_truth_landmarks/bison.mat\")\n",
    "cow_landmark_indices = sio.loadmat(\"data/ground_truth_landmarks/cow.mat\")\n",
    "source_landmark_indices = {k: v for k, v in zip(list(bison_landmark_indices[\"points\"].flatten()), list(bison_landmark_indices[\"centroids\"]))}\n",
    "target_landmark_indices = {k: v for k, v in zip(list(cow_landmark_indices[\"points\"].flatten()), list(cow_landmark_indices[\"centroids\"]))}\n",
    "correspondence_lookup = {k: v for k, v in enumerate(assignment)}\n",
    "common_landmarks = list(set(source_landmark_indices.keys()) & set(target_landmark_indices.keys()))\n",
    "\n",
    "s_l = (np.array(list(source_landmark_indices.values())) - result[\"X\"][\"shift\"].numpy())[:, [0,2,1]]\n",
    "t_l = (np.array(list(target_landmark_indices.values())) - result[\"Y\"][\"shift\"].numpy())[:, [0,2,1]] + np.array([1,0,0])\n",
    "plotter.add_points(s_l, opacity=1., point_size=20, render_points_as_spheres=True, color='r')\n",
    "plotter.add_points(t_l, opacity=1., point_size=20, render_points_as_spheres=True, color='r')\n",
    "\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating evaluation measures\n",
    "\n",
    "Let's evaluate the correspondence model before and after training (and compare this to randomly assigned correspondences)\n",
    "\n",
    "Landmark error:\n",
    "\n",
    "![landmark error](images/landmark_error.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "## landmark based error\n",
    "# 1. Lookup the nearest vertex to each landmark and find the corresponding point in the other shape\n",
    "# 2. Compute the (geodesic) distance between this point and the true nearest vertex to the landmark\n",
    "def landmark_errors(source_landmark_vertices, target_landmark_vertices, assignment, target_geodesic_dists, target_area):\n",
    "    correspondence_lookup = {k: v for k, v in enumerate(assignment)}\n",
    "    common_landmark_indices = list(set(source_landmark_vertices.keys()) & set(target_landmark_vertices.keys()))\n",
    "    errors = []\n",
    "    for landmark_index in common_landmark_indices:\n",
    "        source_landmark = source_landmark_vertices[landmark_index]\n",
    "        target_landmark = target_landmark_vertices[landmark_index]\n",
    "        target_landmark_prime = correspondence_lookup[source_landmark]\n",
    "        error = (1 / np.sqrt(target_area)) * target_geodesic_dists[target_landmark, target_landmark_prime]\n",
    "        errors.append(error)\n",
    "    return errors\n",
    "\n",
    "def get_nearest_vertices_to_landmarks(landmarks, vertices):\n",
    "    kdtree = KDTree(vertices)\n",
    "    nearest_vertices = {}\n",
    "    for landmark_index, landmark in landmarks.items():\n",
    "        nearest_vertices[landmark_index] = kdtree.query(landmark)[1]\n",
    "    return nearest_vertices\n",
    "\n",
    "# Geodesic distance normalised by the square root area of the mesh\n",
    "def get_geodesic_error(geodesic_dists_x, geodesic_dists_y, assignment, area_y, n_sample=10000):\n",
    "    geodesic_dists_x_prime = geodesic_dists_y[assignment]\n",
    "    geodesic_dists_x_prime = geodesic_dists_x_prime[:, assignment]\n",
    "    error = np.abs(geodesic_dists_x - geodesic_dists_x_prime)\n",
    "\n",
    "    # errors normalised by the square root area of the mesh\n",
    "    geodesic_error = (1 / np.sqrt(area_y)) * error\n",
    "    # sample n_sample points\n",
    "    geodesic_error = geodesic_error.reshape(-1)\n",
    "    geodesic_error_sample = np.random.choice(geodesic_error, n_sample, replace=False)\n",
    "    return geodesic_error_sample\n",
    "\n",
    "# load evaluation datasets\n",
    "bison_landmarks = sio.loadmat(\"data/ground_truth_landmarks/bison.mat\")\n",
    "cow_landmarks = sio.loadmat(\"data/ground_truth_landmarks/cow.mat\")\n",
    "bison_vertices = np.asarray(o3d.io.read_triangle_mesh(join(data_folder, \"bison.obj\")).vertices)\n",
    "cow_vertices = np.asarray(o3d.io.read_triangle_mesh(join(data_folder, \"cow.obj\")).vertices)\n",
    "cow_geodesic_dists = np.load(\"data/geodesic_distances/cow.npy\")\n",
    "bison_geodesic_dists = np.load(\"data/geodesic_distances/bison.npy\")\n",
    "cow_area = target_mesh.get_surface_area()\n",
    "\n",
    "bison_landmarks = {k: v for k, v in zip(list(bison_landmarks[\"points\"].flatten()), list(bison_landmarks[\"centroids\"]))}\n",
    "cow_landmarks = {k: v for k, v in zip(list(cow_landmarks[\"points\"].flatten()), list(cow_landmarks[\"centroids\"]))}\n",
    "bison_landmark_vertices = get_nearest_vertices_to_landmarks(bison_landmarks, bison_vertices)\n",
    "cow_landmark_vertices = get_nearest_vertices_to_landmarks(cow_landmarks, cow_vertices)\n",
    "\n",
    "# load the resulting correspondences from the randomly initialised model\n",
    "with open(\"results/random_init_correspondence_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)\n",
    "# compute the errors\n",
    "random_init_errors = landmark_errors(bison_landmark_vertices, cow_landmark_vertices, result[\"assignment\"], cow_geodesic_dists, cow_area)\n",
    "random_init_geodesic_error = get_geodesic_error(bison_geodesic_dists, cow_geodesic_dists, result[\"assignment\"], cow_area)\n",
    "\n",
    "# load the resulting correspondences from the trained model\n",
    "with open(\"results/trained_correspondence_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)\n",
    "# compute the errors\n",
    "trained_errors = landmark_errors(bison_landmark_vertices, cow_landmark_vertices, result[\"assignment\"], cow_geodesic_dists, cow_area)\n",
    "trained_geodesic_error = get_geodesic_error(bison_geodesic_dists, cow_geodesic_dists, result[\"assignment\"], cow_area)\n",
    "\n",
    "# for fun - generate error metrics from randomly assigned correspondences\n",
    "random_assignment = np.random.choice(len(cow_vertices), size=len(bison_vertices))\n",
    "no_model_errors = landmark_errors(bison_landmark_vertices, cow_landmark_vertices, random_assignment, cow_geodesic_dists, cow_area)\n",
    "no_model_geodesic_error = get_geodesic_error(bison_geodesic_dists, cow_geodesic_dists, random_assignment, cow_area)\n",
    "\n",
    "## Quick plotting of the errors\n",
    "fig, (ax0,ax1) = plt.subplots(1,2, figsize=(14,6))\n",
    "\n",
    "# violin plots for the ground truth landmark errors\n",
    "violinplots = ax0.violinplot([no_model_errors, random_init_errors, trained_errors], positions=[1, 1.6, 2.2], showmeans=True,)\n",
    "ax0.set_xticks([1, 1.6, 2.2])\n",
    "ax0.set_xticklabels([\"No model\", \"Random\\ninitialisation\", \"Trained model\"])\n",
    "ax0.set_ylabel(\"Normalised landmark error\")\n",
    "ax0.set_ylim(0)\n",
    "colours = ['red', 'blue', 'green']\n",
    "for i, plot_body in enumerate(violinplots['bodies']):\n",
    "    plot_body.set_facecolor(colours[i])\n",
    "    plot_body.set_edgecolor('black')\n",
    "    plot_body.set_alpha(1)\n",
    "for line in ['cbars','cmins','cmaxes']:\n",
    "    violinplots[line].set_color('black')\n",
    "    violinplots[line].set_linewidth(2)\n",
    "violinplots['cmeans'].set_linestyle('--')\n",
    "violinplots['cmeans'].set_color('white')\n",
    "violinplots['cmeans'].set_linewidth(2)\n",
    "\n",
    "# cumulative distribution of the geodesic errors\n",
    "ax1.plot(np.sort(no_model_geodesic_error), np.linspace(0, 100, len(no_model_geodesic_error)), c=colours[0], label=\"No model\")\n",
    "ax1.plot(np.sort(random_init_geodesic_error), np.linspace(0, 100, len(random_init_geodesic_error)), c=colours[1], label=\"Random init\")\n",
    "ax1.plot(np.sort(trained_geodesic_error), np.linspace(0, 100, len(trained_geodesic_error)), c=colours[2], label=\"Trained\")\n",
    "\n",
    "ax1.set_xlabel(\"Geodesic error\")\n",
    "ax1.set_ylabel(r\"% of matches\")\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.set_xlim(0)\n",
    "ax1.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Limitations and potential solutions\n",
    "\n",
    "While this method is pretty good and works well in some cases, the are others for which it is particularly poor, espcially if there are large differences in the morphology of the two shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of distinct morphological change\n",
    "source_mesh = o3d.io.read_triangle_mesh(\"data/meshes/giraffe.obj\")\n",
    "target_mesh = o3d.io.read_triangle_mesh(\"data/meshes/elephant.obj\")\n",
    "\n",
    "result = run_correpondence_inference(correspondence_model, source_mesh, target_mesh)\n",
    "\n",
    "source_verts = result[\"X\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "target_verts = result[\"Y\"][\"verts\"][:, [0,2,1]] # just reordering x, y, z for visualisation\n",
    "triangles_x = result[\"X\"][\"triangles\"]\n",
    "triangles_y = result[\"Y\"][\"triangles\"]\n",
    "target_verts = target_verts + np.array([2.5,0,0])\n",
    "\n",
    "plotter = pyvista.Plotter(notebook=True)\n",
    "# Forward assignment - set colours on target mesh corresponding with xyz position\n",
    "target_colours = target_verts - np.min(target_verts, axis=0)\n",
    "target_colours = target_colours / np.max(target_colours, axis=0)\n",
    "target_colours = np.concatenate([target_colours, np.ones((len(target_colours), 1))], axis=1)\n",
    "# set colours on source mesh according to the predicted correspondence assignment\n",
    "assignment = result[\"assignment\"]\n",
    "source_colours = target_colours[assignment]\n",
    "\n",
    "plotter.add_points(target_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=target_colours, rgb=True)\n",
    "plotter.add_points(source_verts, opacity=1., point_size=10, render_points_as_spheres=True, scalars=source_colours, rgb=True)\n",
    "plotter.show(jupyter_backend=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset size\n",
    "\n",
    "How about training the model on a much larger dataset! The original Neuromorph model was trained using the FAUST dataset (300 scans of people in different poses). \n",
    "\n",
    "![image of s](images/FAUST_dataset_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network modifications\n",
    "\n",
    " - Enable the network to make null correspondence predictions (Hard!)\n",
    " - Use a more complex algorithm to derive smooth, constrained assignments from the predicted correspondence probabilities (Sinkhorn algorithm)\n",
    " - Use lower quality correspondence predictions as supervision examples in a two stage approach:\n",
    "\n",
    " ![NCP image](images/Neural_correspondence_prior.png)\n",
    "\n",
    " Attaiki and Ovsjanikov 2023, NCP: Neural Correspondence Prior for Effective Unsupervised Shape Matching: https://arxiv.org/abs/2301.05839 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification for use with medical images/shapes\n",
    "\n",
    "Application to radiotherapy: able to leverage extra information - the associated medical imaging (CT scans)\n",
    "\n",
    "Doing so allowed the model more context to help identify real corresponding anatomy.\n",
    "\n",
    "![rad image](images/neuromorph_plus_imaging.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eds_dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
